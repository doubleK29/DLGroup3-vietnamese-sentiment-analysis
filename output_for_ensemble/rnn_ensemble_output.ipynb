{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn_ensemble_output.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KCgy8FmV5elJ","executionInfo":{"status":"ok","timestamp":1641138024763,"user_tz":-420,"elapsed":22246,"user":{"displayName":"Plastic Dude","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsVOBFFvIv6SiPy0d4xeCb0Kz66XkfinI9MHM_aw=s64","userId":"00552327576249957449"}},"outputId":"36a0e5df-5a75-4e80-f38d-b2b678b3541d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import pickle"],"metadata":{"id":"JGG5HQSh5ljI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"UOqTO3He5tRV"}},{"cell_type":"code","source":["train_path = 'drive//MyDrive/DeepLearningProject_Group3/_UIT-VSFC/train'\n","dev_path = 'drive/MyDrive/DeepLearningProject_Group3/_UIT-VSFC/dev'\n","test_path = 'drive/MyDrive/DeepLearningProject_Group3/_UIT-VSFC/test'\n","\n","with open(os.path.join(train_path, 'sents.txt')) as f:\n","  train_raw = f.read().splitlines()\n","with open(os.path.join(train_path, 'sentiments.txt')) as f:\n","  train_sentiments = list(map(int, f.read().splitlines()))\n","with open(os.path.join(train_path, 'topics.txt')) as f:\n","  train_topics = list(map(int, f.read().splitlines()))\n","\n","with open(os.path.join(dev_path, 'sents.txt')) as f:\n","  dev_raw = f.read().splitlines()\n","with open(os.path.join(dev_path, 'sentiments.txt')) as f:\n","  dev_sentiments = list(map(int, f.read().splitlines()))\n","with open(os.path.join(dev_path, 'topics.txt')) as f:\n","  dev_topics = list(map(int, f.read().splitlines()))\n","\n","with open(os.path.join(test_path, 'sents.txt')) as f:\n","  test_raw = f.read().splitlines()\n","with open(os.path.join(test_path, 'sentiments.txt')) as f:\n","  test_sentiments = list(map(int, f.read().splitlines()))\n","with open(os.path.join(test_path, 'topics.txt')) as f:\n","  test_topics = list(map(int, f.read().splitlines()))"],"metadata":{"id":"yfK8mOHI50Fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if True:\n","  train_path = 'drive//MyDrive/DeepLearningProject_Group3/segmented_data'\n","  dev_path = 'drive/MyDrive/DeepLearningProject_Group3/segmented_data'\n","  test_path = 'drive/MyDrive/DeepLearningProject_Group3/segmented_data'\n","\n","  train = pd.read_csv(os.path.join(train_path, 'train_segmented.csv'))\n","  train_raw = train['sents'].tolist()\n","  train_sentiments = train['labels'].tolist()\n","  train_topics = train['topics'].tolist()\n","\n","  dev = pd.read_csv(os.path.join(dev_path, 'dev_segmented.csv'))\n","  dev_raw = dev['sents'].tolist()\n","  dev_sentiments = dev['labels'].tolist()\n","  dev_topics = dev['topics'].tolist()\n","\n","  test = pd.read_csv(os.path.join(test_path, 'test_segmented.csv'))\n","  test_raw = test['sents'].tolist()\n","  test_sentiments = test['labels'].tolist()\n","  test_topics = test['topics'].tolist()"],"metadata":{"id":"siPAz-kfGDq6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","vocabulary = {}\n","vocabulary['<pad>'] = count\n","count += 1\n","vocabulary['<unknown>'] = count\n","for sentence in train_raw:\n","  words = sentence.split()\n","  for word in words:\n","    if 'wzjwz' in word:\n","      word = '<name>'\n","    if word not in vocabulary.keys():\n","      vocabulary[word] = count\n","      count += 1"],"metadata":{"id":"8PzCePh_50uJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenizer_encode(raw_data, vocabulary):\n","  encoding = []\n","  for sentence in raw_data:\n","    words = sentence.split()\n","    sentence_te = []\n","    for word in words:\n","      try:\n","        sentence_te.append(vocabulary[word])\n","      except:\n","        if 'wzjwz' in word:\n","          sentence_te.append(vocabulary['<name>'])\n","        else:\n","          sentence_te.append(vocabulary['<unknown>'])\n","    encoding.append(sentence_te)\n","  return encoding"],"metadata":{"id":"6FndT5VB52hf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_encoded = tokenizer_encode(dev_raw, vocabulary)\n","test_encoded = tokenizer_encode(test_raw, vocabulary)"],"metadata":{"id":"-QIDExET54lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenizer_encoding_pad(data, vocabulary, seq_length):\n","  for sentence in data:\n","    pad_length = seq_length - len(sentence)\n","    if pad_length > 0:\n","      for i in range(pad_length):\n","        sentence.append(vocabulary['<pad>'])\n","    elif pad_length < 0:\n","      for i in range(-pad_length):\n","        sentence.pop()\n","  return data"],"metadata":{"id":"xFmNCwci5-tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_encoded = tokenizer_encoding_pad(dev_encoded, vocabulary, seq_length=14)\n","test_encoded = tokenizer_encoding_pad(test_encoded, vocabulary, seq_length=14)"],"metadata":{"id":"nC7C1y1G6A4O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SentenceDataset(Dataset):\n","  def __init__(self, sents, sentiments, topics):\n","    super(SentenceDataset, self).__init__()\n","    self.sents = torch.FloatTensor(sents)\n","    if len(self.sents.shape) == 2:\n","      self.sents = self.sents.reshape((self.sents.shape[0],\n","                                       self.sents.shape[1],\n","                                       1))\n","    self.sentiments = torch.tensor(sentiments, dtype=torch.long)\n","    self.topics = torch.tensor(topics, dtype=torch.long)\n","\n","  def __len__(self):\n","    return len(self.sentiments)\n","  \n","  def __getitem__(self, idx):\n","    return self.sents[idx], self.sentiments[idx], self.topics[idx]"],"metadata":{"id":"t_S6UdVQ6DXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnn_dev_dataset = SentenceDataset(sents=dev_encoded,\n","                                  sentiments=dev_sentiments,\n","                                  topics=dev_topics)\n","rnn_test_dataset = SentenceDataset(sents=test_encoded,\n","                                   sentiments=test_sentiments,\n","                                   topics=test_topics)"],"metadata":{"id":"obQEpt096GEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnn_dev_loader = DataLoader(rnn_dev_dataset,\n","                            batch_size=128)\n","rnn_test_loader = DataLoader(rnn_test_dataset,\n","                             batch_size=128)"],"metadata":{"id":"gPZxXUIC6LK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"EMADN3lH6Nw3"}},{"cell_type":"code","source":["class DeepRNN(nn.Module):\n","  def __init__(self, input_size, hidden_size, output_size, \n","               embedding, vocab_size, embedding_dim):\n","    super(DeepRNN, self).__init__()\n","    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.embedding = embedding\n","    self.embedding_dim = embedding_dim\n","    if self.embedding:\n","      assert self.input_size == 1, 'Input must be a list of indices'\n","      assert isinstance(vocab_size, int), 'vocab_size must be int'\n","      assert isinstance(embedding_dim, int), 'embedding_dim must be int'\n","      self.embedding_layer = nn.Embedding(vocab_size, embedding_dim,\n","                                          max_norm=1, device='cuda')\n","      self.input_size = embedding_dim\n","    self.input_to_hidden = nn.Linear(self.input_size,\n","                                     self.hidden_size,\n","                                     device=self.device)\n","    self.h2h_same_layer_1 = nn.Linear(self.hidden_size,\n","                                      self.hidden_size,\n","                                      device=self.device)\n","    self.h2h_between_layers_12 = nn.Linear(self.hidden_size,\n","                                           self.hidden_size,\n","                                           device=self.device)\n","    self.h2h_same_layer_2 = nn.Linear(self.hidden_size,\n","                                      self.hidden_size,\n","                                      device=self.device)\n","    self.h2h_between_layers_23 = nn.Linear(self.hidden_size,\n","                                           self.hidden_size,\n","                                           device=self.device)\n","    self.h2h_same_layer_3 = nn.Linear(self.hidden_size,\n","                                      self.hidden_size,\n","                                      device=self.device)\n","    self.hidden_to_output = nn.Linear(self.hidden_size,\n","                                      self.output_size,\n","                                      device=self.device)\n","    self.tanh = nn.Tanh()\n","    self.softmax = nn.LogSoftmax(dim=1)\n","\n","  def forward(self, input_tensor):\n","    if self.embedding:\n","      input_tensor = self.embedding_layer(input_tensor.long()).squeeze()\n","    # Each layer has a hidden state\n","    hidden_1 = torch.zeros((1, self.hidden_size), device=self.device)\n","    hidden_2 = torch.zeros((1, self.hidden_size), device=self.device)\n","    hidden_3 = torch.zeros((1, self.hidden_size), device=self.device)\n","    for word in input_tensor:\n","      hidden_1 = self.input_to_hidden(word) + self.h2h_same_layer_1(hidden_1)\n","      hidden_1 = self.tanh(hidden_1)\n","      hidden_2 = self.h2h_between_layers_12(hidden_1) + self.h2h_same_layer_2(hidden_2)\n","      hidden_2 = self.tanh(hidden_2)\n","      hidden_3 = self.h2h_between_layers_23(hidden_2) + self.h2h_same_layer_3(hidden_3)\n","      hidden_3 = self.tanh(hidden_3)\n","      output = self.hidden_to_output(hidden_3)\n","    # Only the output of the last RNN cell is taken into account\n","    output = self.softmax(output)\n","    # The log-softmax function combined with negative log likelihood loss\n","    # gives the same effect as cross entropy loss taken straight from the output\n","    return output\n","\n","  def predict(self, input_tensor):\n","    with torch.no_grad():\n","      prediction = torch.argmax(self.forward(input_tensor))\n","    return prediction"],"metadata":{"id":"g9lZ2_V66O7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnn_path = '/content/drive/MyDrive/DeepLearningProject_Group3/checkpoint/RNN/DeepRNN_word_segmented.pth'\n","model = torch.load(rnn_path)"],"metadata":{"id":"U6NK0Tx46Q-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, data_loader):\n","  prediction = None\n","  with torch.no_grad():\n","      for data, sentiments, _ in data_loader:\n","        data = data.transpose(0, 1)\n","        data, sentiments = data.to('cuda'), sentiments.to('cuda')\n","        output = model(data)\n","        if prediction is not None:\n","          prediction = np.concatenate((prediction, output.cpu().detach().numpy()))\n","        else:\n","          prediction = output.cpu().detach().numpy()\n","  return prediction"],"metadata":{"id":"Pr1baD8k7OQw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_dev = predict(model, rnn_dev_loader)\n","prediction_dev = np.exp(prediction_dev)\n","print(prediction_dev)\n","print(len(prediction_dev))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGP_YdbcJgup","executionInfo":{"status":"ok","timestamp":1641138048214,"user_tz":-420,"elapsed":504,"user":{"displayName":"Plastic Dude","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsVOBFFvIv6SiPy0d4xeCb0Kz66XkfinI9MHM_aw=s64","userId":"00552327576249957449"}},"outputId":"ce3feb6d-3796-400e-c08f-f7afe841965e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[9.7997105e-01 1.0502942e-02 9.5259724e-03]\n"," [9.6513635e-01 1.5109286e-02 1.9754410e-02]\n"," [4.3614348e-04 1.0246724e-03 9.9853927e-01]\n"," ...\n"," [9.7035313e-01 7.8405086e-03 2.1806464e-02]\n"," [9.3891108e-01 1.7092953e-02 4.3995999e-02]\n"," [1.8006451e-01 2.2772828e-01 5.9220713e-01]]\n","1583\n"]}]},{"cell_type":"code","source":["prediction_test = predict(model, rnn_test_loader)\n","prediction_test = np.exp(prediction_test)\n","print(prediction_test)\n","print(len(prediction_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XXNJQjf7zFN","executionInfo":{"status":"ok","timestamp":1641138048518,"user_tz":-420,"elapsed":306,"user":{"displayName":"Plastic Dude","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsVOBFFvIv6SiPy0d4xeCb0Kz66XkfinI9MHM_aw=s64","userId":"00552327576249957449"}},"outputId":"0217293d-6cae-4084-89b6-4801f483554f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.03012835 0.0726084  0.8972632 ]\n"," [0.00106289 0.00374366 0.9951935 ]\n"," [0.00321643 0.00705959 0.9897239 ]\n"," ...\n"," [0.00139087 0.00499388 0.99361527]\n"," [0.18806262 0.24549697 0.5664404 ]\n"," [0.03720106 0.02326983 0.9395291 ]]\n","3166\n"]}]},{"cell_type":"code","source":["def save_output(output, file_path):\n","  with open(file_path, \"wb\") as file:\n","      pickle.dump(output, file)"],"metadata":{"id":"h9d3T7ms81-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if True:\n","  test_path = \"drive/MyDrive/DeepLearningProject_Group3/output_for_ensemble/rnn_output_test_word_segmented.pkl\"\n","  save_output(prediction_test, test_path)\n","  dev_path = \"drive/MyDrive/DeepLearningProject_Group3/output_for_ensemble/rnn_output_dev_word_segmented.pkl\"\n","  save_output(prediction_dev, dev_path)"],"metadata":{"id":"Bkjfz45IJt72"},"execution_count":null,"outputs":[]}]}