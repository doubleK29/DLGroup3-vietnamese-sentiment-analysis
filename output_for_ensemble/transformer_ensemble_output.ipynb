{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_ensemble_output.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bHhJjggS9Xf1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641134239482,"user_tz":-420,"elapsed":2602,"user":{"displayName":"Hoàng Nguyễn Việt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06587487919262329307"}},"outputId":"55ec4203-f759-4641-89ff-c3fbbe8dee4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn, Tensor\n","import json\n","import math\n","\n","import numpy as np\n","import pickle\n","batch_size = 64"],"metadata":{"id":"_lE-s2SwRqbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZudVCvlRT6H","executionInfo":{"status":"ok","timestamp":1641134245589,"user_tz":-420,"elapsed":4141,"user":{"displayName":"Hoàng Nguyễn Việt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06587487919262329307"}},"outputId":"c4df9b3d-89cc-4dae-ec96-48821c3d073b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1-J0uZgsbCvlTa7eu9VQ89V_onUXVy_eW\n","To: /content/preprocessedForTransformer.zip\n","100% 8.11M/8.11M [00:00<00:00, 71.5MB/s]\n"]}],"source":["!gdown --id 1-J0uZgsbCvlTa7eu9VQ89V_onUXVy_eW"]},{"cell_type":"code","source":["! unzip \"preprocessedForTransformer.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8HtZVItRfN9","executionInfo":{"status":"ok","timestamp":1641134255773,"user_tz":-420,"elapsed":10188,"user":{"displayName":"Hoàng Nguyễn Việt","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06587487919262329307"}},"outputId":"f138c11d-5bb0-474c-902d-dfae84c5886f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  preprocessedForTransformer.zip\n","replace augmented_pretrain.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["with open('dev.json', 'r') as f: \n","    dev_data1 = json.load(f)\n","with open('test.json', 'r') as f: \n","    test_data1 = json.load(f)\n","\n","with open('word_segmented_dev.json', 'r') as f: \n","    dev_data2 = json.load(f)\n","with open('word_segmented_test.json', 'r') as f: \n","    test_data2 = json.load(f)"],"metadata":{"id":"INfLjrJqSmCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_tokens_tensor1 = torch.tensor(dev_data1['tokens'], dtype=torch.long)\n","dev_y_tensor1 = torch.tensor(dev_data1['sentiments']).long()\n","\n","dev_tokens_tensor2 = torch.tensor(dev_data2['tokens'], dtype=torch.long)\n","dev_y_tensor2 = torch.tensor(dev_data2['sentiments']).long()\n","\n","\n","test_tokens_tensor1 = torch.tensor(test_data1['tokens'], dtype=torch.long)\n","test_y_tensor1 = torch.tensor(test_data1['sentiments']).long()\n","\n","test_tokens_tensor2 = torch.tensor(test_data2['tokens'], dtype=torch.long)\n","test_y_tensor2 = torch.tensor(test_data2['sentiments']).long()"],"metadata":{"id":"gzAI2AS-Szxp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from typing import Tuple\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset\n","\n","\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n","                 nlayers: int, dropout: float, sequence_size: int, class_num: int):\n","        super().__init__()\n","        self.model_type = 'Transformer'\n","        self.pos_encoder = PositionalEncoding(d_model, dropout, sequence_size)\n","        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, d_model)\n","        self.d_model = d_model\n","        self.decoder = nn.Linear(d_model, ntoken)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = nn.Linear(d_model, class_num)\n","        # self.avgpool = nn.AvgPool1d(sequence_size)\n","        self.weighted_pool = nn.Linear(sequence_size,1)\n","        self.init_weights()\n","\n","    def init_weights(self) -> None:\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src: Tensor, pretraining = False) -> Tensor:\n","        src = self.encoder(src) * math.sqrt(self.d_model)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src)\n","        if pretraining:\n","          output = self.decoder(output)\n","        else:\n","          output = self.linear(output)\n","          # output = self.avgpool(output.permute(2,1,0)).permute(2,1,0)\n","          output = self.weighted_pool(output.permute(2,1,0)).permute(2,1,0)\n","        output = F.log_softmax(output,dim =-1)\n","        return output.squeeze()\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n","        "],"metadata":{"id":"Cm9kuRzYgq4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_pred(model, dir, test_tokens_tensor):\n","    full_dir = 'drive/MyDrive/DeepLearningProject_Group3/output_for_ensemble/'+dir\n","    predicted = []\n","    model1.eval()\n","    with torch.no_grad():\n","        token_ids = torch.transpose(test_tokens_tensor,0,1)\n","\n","        logits = model(token_ids)\n","    \n","    a = logits.numpy()\n","    a = np.exp(a)\n","    with open(full_dir, \"wb\") as file:\n","        pickle.dump(a, file)"],"metadata":{"id":"KbXb1rRDS5AS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_dir1 = 'transformer_output_dev.pkl'\n","test_dir1 = 'transformer_output_test.pkl'\n","\n","dev_dir2 = 'transformer_output_word_segmented_dev.pkl'\n","test_dir2 = 'transformer_output_word_segmented_test.pkl'\n","\n","path1 = '/content/drive/MyDrive/DeepLearningProject_Group3/checkpoint/Transformer/Transformer.pth'\n","model1 = torch.load(path1, map_location=torch.device('cpu'))\n","\n","path2 = '/content/drive/MyDrive/DeepLearningProject_Group3/checkpoint/Transformer/Transformer_word_segmented.pth'\n","model2 = torch.load(path2, map_location=torch.device('cpu'))\n","\n","save_pred(model1, dev_dir1, dev_tokens_tensor1)\n","save_pred(model1, test_dir1, test_tokens_tensor1)\n","save_pred(model2, dev_dir2, dev_tokens_tensor2)\n","save_pred(model2, test_dir2, test_tokens_tensor2)"],"metadata":{"id":"Y6UjfQSdGxqB"},"execution_count":null,"outputs":[]}]}